{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchtext give us a lot of ease but sometimes we need to create customer dataset\n",
    "#we will be creating image captioning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to numerical values\n",
    "#need vocab mapping\n",
    "#setup pytorch dataset\n",
    "#setup padding of every batch as, all datapoints should be of same seq_len in a batch and setup data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy #for tokenization\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold): #if word is spoken less than the frequency then thats not useful for us, so we ignore that\n",
    "        self.itos= {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'} #itos means index to string\n",
    "        self.stoi= {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold= freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod #it means fucntion dont have access to instance or class attributes, used for like utility functions\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)] #using spacy to tokenize, can also use split with space but its better\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list): #all captions in dataset is passed so can create vocab and keep track of their freq and start index from 4 as till 3 given\n",
    "        frequencies= {}\n",
    "        idx= 4\n",
    "\n",
    "        for i in sentence_list:\n",
    "            for word in self.tokenizer_eng(i):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word]= 1\n",
    "                else:\n",
    "                    frequencies[word]+= 1\n",
    "\n",
    "                if frequencies[word]== self.freq_threshold:\n",
    "                    self.stoi[word]= idx\n",
    "                    self.itos[idx]= word\n",
    "                    idx+=1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text= self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[word] if word in self.stoi else self.stoi['<UNK>']\n",
    "            for word in tokenized_text\n",
    "        ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform= None, freq_theshold= 5):\n",
    "        self.root_dir= root_dir\n",
    "        self.df= pd.read_csv(captions_file)\n",
    "        self.transform= transform\n",
    "\n",
    "        #get image and cpation columns\n",
    "        self.imgs= self.df['image']\n",
    "        self.captions= self.df['caption']\n",
    "\n",
    "        #initialize vocab and build it\n",
    "        self.vocab= Vocabulary(freq_theshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption= self.captions[index]\n",
    "        img_id= self.imgs[index]\n",
    "        img= Image.open(os.path.join(self.root_dir, img_id)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img= self.transform(img)\n",
    "\n",
    "        numericalize_caption= [self.vocab.stoi['<SOS>']] #stoi is string to index, we first create for start token\n",
    "        numericalize_caption+= self.vocab.numericalize(caption)\n",
    "        numericalize_caption.append(self.vocab.stoi['<EOS>'])\n",
    "\n",
    "        return img, torch.tensor(numericalize_caption)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want that in a batch all text(caption) should be of same length\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx= pad_idx\n",
    "\n",
    "    def __call__(self, batch): #we get list of list that contain img and caption\n",
    "        imgs= [item[0].unsqueeze(0) for item in batch] #need extra dimension for the batch\n",
    "        imgs= torch.cat(imgs, dim= 0)\n",
    "        targets= [item[1] for item in batch]\n",
    "        targets= pad_sequence(targets, batch_first= False, padding_value= self.pad_idx)\n",
    "        #batch_first is set according to model requirement that what we want first\n",
    "        #False (default): Returns a tensor of shape (max_seq_length, batch_size, features)\n",
    "        #True: Returns a tensor of shape (batch_size, max_seq_length, features)\n",
    "\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "        root_folder,\n",
    "        annotation_file,\n",
    "        transform,\n",
    "        batch_size= 32,\n",
    "        num_workers= 4,\n",
    "        shuffle= True, #if working with time series data then we dont shuffle\n",
    "        pin_memory= True,\n",
    "):\n",
    "    dataset= FlickerDataset(root_folder,\n",
    "                            annotation_file,\n",
    "                            transform\n",
    "                            )\n",
    "    \n",
    "    pad_idx= dataset.vocab.stoi['<PAD>']\n",
    "\n",
    "    loader= DataLoader(\n",
    "        dataset= dataset,\n",
    "        batch_size= batch_size,\n",
    "        #num_workers= num_workers,\n",
    "        shuffle= shuffle,\n",
    "        pin_memory= pin_memory,\n",
    "        collate_fn= MyCollate(pad_idx= pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform= transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataloader= get_loader('data/flicker8k/Images', 'data/flicker8k/captions.txt', transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([21, 32])\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([23, 32])\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([22, 32])\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([24, 32])\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([24, 32])\n",
      "torch.Size([32, 3, 224, 224]) torch.Size([21, 32])\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(dataloader):\n",
    "    print(i[0].shape, i[1].shape)\n",
    "    if idx==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
